{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "We define the feature matrix as:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\left[\\begin{array}{ccc}\n",
    "\\text{cos}x & \\text{sin}x & 1\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{n \\times 3} ,\n",
    "$$\n",
    "\n",
    "and the parameter vector as:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta} = \\left[\\begin{array}{ccc}\n",
    "\\alpha & \\beta & \\gamma\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{3}.\n",
    "$$\n",
    "\n",
    "The function can be then rewritten as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X}\\mathbf{\\theta}^\\top.\n",
    "$$\n",
    "\n",
    "To find the optimal estimate of $\\theta$ (represented as $\\hat{\\mathbf{\\theta}}$) as a least square problem, it can be formulated as:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{\\theta}} = \\underset{\\theta}{\\mathbf{\\text{argmin}}} \\left\\| \\mathbf{y} - \\mathbf{X} \\mathbf{\\theta}^\\top \\right\\|_2^2,\n",
    "$$\n",
    "\n",
    "and according to the lecture note, the closed form of solution is given by\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{\\theta}} = (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "### 1.\n",
    "\n",
    "Given $\\mathbf{z} \\sim \\mathcal{N} (0, \\sigma^2)$, we have $\\mathbb{E} (\\mathbf{z}) = 0$.\n",
    "\n",
    "The closed form of solution of $\\hat{\\mathbf{\\theta}}$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "  \\hat{\\mathbf{\\theta}} &= (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{y} \\\\\n",
    "                        &= (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} (\\mathbf{y}^* + \\mathbf{z}) \\\\\n",
    "                        &= (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{y}^* + (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{z} \\\\\n",
    "                        &= \\mathbf{\\theta}^* + (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{z}, \n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "therefore, we can calculate the expectation of $\\hat{\\mathbf{\\theta}}$ as:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "  \\mathbb{E}(\\hat{\\mathbf{\\theta}}) &= \\mathbb{E}(\\mathbf{\\theta}^* + (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{z}) \\\\\n",
    "                                    &= \\mathbf{\\theta}^* + (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbb{E}(\\mathbf{z}),\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "recall that we have $\\mathbb{E} (\\mathbf{z}) = 0$, which leads to\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\hat{\\mathbf{\\theta}}) = \\mathbf{\\theta}^*.\n",
    "$$\n",
    "\n",
    "To further prove the next equation, firstly utilize the given hint to reformulate the left side of the equation\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "  \\mathbb{E} (\\left\\| \\mathbf{y}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}} \\right\\|_2^2) &= \\mathbb{E} (\\left\\| \\mathbf{y}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}} + \\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2) \\\\\n",
    "  &= \\mathbb{E} (\\left\\| (\\mathbf{y}^* - \\mathbf{X} \\mathbf{\\theta}^*) + (\\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}}) \\right\\|_2^2) \\\\\n",
    "  &= \\mathbb{E} (\\left\\| \\mathbf{y}^* - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2 + \\left\\| \\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}} \\right\\|_2^2 -2<\\mathbf{y}^* - \\mathbf{X} \\mathbf{\\theta}^*, \\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}}>) \\\\\n",
    "  &= \\mathbb{E} (\\left\\| \\mathbf{y}^* - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2) + \\mathbb{E} (\\left\\| \\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}} \\right\\|_2^2) - 2\\mathbb{E} ((\\mathbf{y}^* - \\mathbf{X} \\mathbf{\\theta}^*)^{\\top} \\cdot (\\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}})) \\\\\n",
    "  &= \\mathbb{E} (\\left\\| \\mathbf{y}^* - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2) + \\mathbb{E} (\\left\\| \\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}} \\right\\|_2^2) - 2\\mathbb{E} (\\mathbf{y}^{* \\top} \\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{y}^{* \\top} \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^{* \\top} \\mathbf{X}^{\\top} \\mathbf{X} \\mathbf{\\theta}^* + \\mathbf{\\theta}^{* \\top} \\mathbf{X}^{\\top} \\mathbf{X} \\hat{\\mathbf{\\theta}}) \\\\\n",
    "  &= \\mathbb{E} (\\left\\| \\mathbf{y}^* - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2) + \\mathbb{E} (\\left\\| \\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}} \\right\\|_2^2) - 2(\\mathbf{y}^{* \\top} \\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{y}^{* \\top} \\mathbf{X} \\mathbb{E} (\\hat{\\mathbf{\\theta}}) - \\mathbf{\\theta}^{* \\top} \\mathbf{X}^{\\top} \\mathbf{X} \\mathbf{\\theta}^* + \\mathbf{\\theta}^{* \\top} \\mathbf{X}^{\\top} \\mathbf{X} \\mathbb{E} (\\hat{\\mathbf{\\theta}})).\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Recall that we have already prooven that $\\mathbb{E}(\\hat{\\mathbf{\\theta}}) = \\mathbf{\\theta}^*$, which makes that all the terms in the 3.rd bracket cancell each other, i.e.,\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "  \\mathbb{E} (\\left\\| \\mathbf{y}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}} \\right\\|_2^2) &= \\mathbb{E} (\\left\\| \\mathbf{y}^* - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2) + \\mathbb{E} (\\left\\| \\mathbf{X} \\mathbf{\\theta}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}} \\right\\|_2^2) \\\\\n",
    "  &= \\left\\| \\mathbf{y}^* - \\mathbb{E} (\\mathbf{X} \\mathbf{\\theta}^*) \\right\\|_2^2 + \\mathbb{E} (\\left\\| \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2)\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "### 2.\n",
    "\n",
    "Recall that we have derived the closed form of solution of $\\hat{\\mathbf{\\theta}}$ as $\\hat{\\mathbf{\\theta}} = \\mathbf{\\theta}^* + (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{z}$, we can see that as $\\mathbf{z} \\sim \\mathcal{N} (0, \\sigma^2)$, therefore $\\hat{\\mathbf{\\theta}}$ is also gaussian distributed, where the mean is by $\\mathbf{\\theta}^*$ shifted.\n",
    "\n",
    "Then we only have to focus on the second term in $\\hat{\\mathbf{\\theta}}$, i.e., $(\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{z}$. According to the task, given $\\mathbf{v} \\sim \\mathcal{N}(0, \\mathbf{\\Sigma})$, then $\\mathbf{A}\\mathbf{v} \\sim (0,\\mathbf{A} \\mathbf{\\Sigma} \\mathbf{A}^{\\top})$. Let $\\mathbf{A} = (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top}$ and $\\mathbf{\\Sigma} = \\sigma^2$, then we have for the variance\n",
    "$$\n",
    "\\sigma^2 (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} ((\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top})^\\top = \\sigma^2 (\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\n",
    "$$\n",
    "\n",
    "Therefore, $\\hat{\\mathbf{\\theta}} \\sim \\mathcal{N} (\\mathbf{\\theta}^*, \\sigma^2 (\\mathbf{X}^{\\top} \\mathbf{X})^{-1})$\n",
    "\n",
    "### 3.\n",
    "\n",
    "For the given equation:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "\\frac{1}{n}\\mathbb{E} \\left[ \\left\\| \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2 \\right] &= \\sigma^2 \\frac{d}{n} \\\\\n",
    "\\mathbb{E} \\left[ \\left\\| \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2 \\right] &= \\sigma^2 d.\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "Now, let's focus on the left side of the equation\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "\\text{left side} &= \\mathbb{E} \\left[ \\left< \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{X} \\mathbf{\\theta}^* , \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{X} \\mathbf{\\theta}^* \\right> \\right] \\\\\n",
    "&= \\mathbb{E} \\left[ \\left( \\mathbf{X} \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\right)^{\\top} \\left( \\mathbf{X} \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\right) \\right] \\\\\n",
    "&= \\mathbb{E} \\left[ \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\mathbf{X}^{\\top} \\mathbf{X} \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\right]. \\\\\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "We know that the result of a inner product has to be a scalar, thus we have for the terms inside the expectation operator\n",
    "\n",
    "$$\n",
    "\\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\mathbf{X}^{\\top} \\mathbf{X} \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) = \\text{trace} \\left( \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\mathbf{X}^{\\top} \\mathbf{X} \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\right),\n",
    "$$\n",
    "\n",
    "and therefore the expectation value can be calculated with the help of trace\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\mathbf{X}^{\\top} \\mathbf{X} \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\right] = \\mathbb{E} \\left[ \\text{trace} \\left( \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\mathbf{X}^{\\top} \\mathbf{X} \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\right) \\right].\n",
    "$$\n",
    "\n",
    "Given $\\mathbb{E} \\left[ \\text{trace} \\left(\\mathbf{A}\\mathbf{B}\\mathbf{C}\\right) \\right] = \\mathbb{E} \\left[ \\text{trace} \\left(\\mathbf{C}\\mathbf{A}\\mathbf{B}\\right) \\right]$, let $\\mathbf{A} = \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top}$, $\\mathbf{B} = \\mathbf{X}^{\\top} \\mathbf{X}$, and $\\mathbf{C} = \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)$, we can re-formulate the right side of equation above as\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\text{trace} \\left( \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\mathbf{X}^{\\top} \\mathbf{X} \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\right) \\right] = \\mathbb{E} \\left[ \\text{trace} \\left( \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\mathbf{X}^{\\top} \\mathbf{X} \\right) \\right]\n",
    "$$\n",
    "\n",
    "and according to $\\mathbb{E} \\left[ \\text{trace} \\left( \\mathbf{A} \\right) \\right] = \\text{trace} \\left( \\mathbb{E} \\left[ \\mathbf{A} \\right] \\right)$, the equation can be further rewritten as:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "\\mathbb{E} \\left[ \\text{trace} \\left( \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\mathbf{X}^{\\top} \\mathbf{X} \\right) \\right] &= \\text{trace} \\left( \\mathbb{E} \\left[ \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\mathbf{X}^{\\top} \\mathbf{X} \\right] \\right) \\\\\n",
    "&= \\text{trace} \\left( \\mathbb{E} \\left[ \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\right] \\mathbf{X}^{\\top} \\mathbf{X} \\right)\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "we can see that the term $\\mathbb{E} \\left[ \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\right]$ is the covariance matrix of $\\hat{\\mathbf{\\theta}}$ since that $\\mathbb{E} \\left[ \\hat{\\mathbf{\\theta}} \\right] = \\mathbf{\\theta}^*$, and we also had $\\hat{\\mathbf{\\theta}} \\sim \\mathcal{N} \\left( \\mathbf{\\theta}^*, \\sigma^2 \\left( \\mathbf{X}^{\\top} \\mathbf{X} \\right)^{-1} \\right)$, therefore, $\\mathbb{E} \\left[ \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\right] = \\sigma^2 \\left( \\mathbf{X}^{\\top} \\mathbf{X} \\right)^{-1}$. So for the right side of the equation above we have:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "\\text{trace} \\left( \\mathbb{E} \\left[ \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right) \\left( \\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}^* \\right)^{\\top} \\right] \\mathbf{X}^{\\top} \\mathbf{X} \\right) &= \\text{trace} \\left( \\sigma^2 \\left( \\mathbf{X}^{\\top} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{X} \\right) \\\\\n",
    "&= \\sigma^2 \\text{trace} \\left( \\mathbf{I} \\right) \\\\\n",
    "&= \\sigma^2 d.\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "It is proven that\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\left\\| \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2 \\right] = \\mathbb{E} \\left[ \\left< \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{X} \\mathbf{\\theta}^* , \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{X} \\mathbf{\\theta}^* \\right> \\right] = \\sigma^2 d,\n",
    "$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\mathbb{E} \\left[ \\left\\| \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2 \\right] = \\sigma^2 \\frac{d}{n}.\n",
    "$$\n",
    "\n",
    "### 4.\n",
    "\n",
    "Given assumption that the underlying function is linear, i.e., all the terms with order higher than 1 in the feature vector should be multiplied by 0 in the parameter vector, which gives us the parameter vector $\\mathbf{\\theta}$ as $\\mathbf{\\theta} = \\left[ \\begin{array}{ccc} \\theta_0 & \\theta_1 & \\mathbf{0}_{D-1} \\end{array} \\right] \\in \\mathbb{R}^{D+1}$.\n",
    "\n",
    "And $\\left\\| \\mathbf{y}^* - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2 = 0$.\n",
    "\n",
    "For this subtask we have $d = D+1$, and according to previous task, where we proven that $\\frac{1}{n} \\mathbb{E} \\left[ \\left\\| \\mathbf{X} \\hat{\\mathbf{\\theta}} - \\mathbf{X} \\mathbf{\\theta}^* \\right\\|_2^2 \\right] = \\sigma^2 \\frac{d}{n}$, we have\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\mathbb{E} \\left[ \\left\\| \\mathbf{y}^* - \\mathbf{X} \\hat{\\mathbf{\\theta}} \\right\\|_2^2 \\right] = \\sigma^2 \\frac{D+1}{n}.\n",
    "$$\n",
    "\n",
    "$\\sigma^2 \\frac{D+1}{n}$ should be upper bounded by $\\epsilon$, which means\n",
    "\n",
    "$$\n",
    "\\sigma^2 \\frac{D+1}{n} \\le \\epsilon ~~~~~~ \\Rightarrow ~~~~~~ n \\ge \\sigma^2 \\frac{D+1}{\\epsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
