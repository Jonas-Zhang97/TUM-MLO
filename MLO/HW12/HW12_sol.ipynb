{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "We firstly consider a single head attention setting for simplicity.\n",
    "\n",
    "The input sequence is embeded to generate an input matrix. denoted as $\\mathbf{X} \\in \\mathbb{R}^{\\text{context\\_length}\\times\\text{dim\\_embd}}$. This matrix is then passed to a weight matrix that is learned, we denote the weight matrix as $\\mathbf{W}$ with size $\\mathbb{R}^{\\text{dim\\_embd}\\times 3\\cdot \\text{dim\\_embd}}$. According to the problem setting, we have $\\text{n\\_layers}$ many layers, which give us $\\text{n\\_layers} \\cdot 3 \\cdot\\text{dim\\_embd}^{2}$ many trainable parameters in a single head.\n",
    "\n",
    "Now, we come back to the given setting, where we have multihead self attention, i.e., $\\text{n\\_heads}$ many heads, which lead to the total number trainable parameters as $\\text{n\\_layers} \\cdot 3 \\cdot \\text{dim\\_embd}^{2} \\cdot \\text{n\\_heads}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "We begin again with a single head attention setting.\n",
    "\n",
    "After embedding, a token is transformed into a vector in $\\mathbb{R}^{\\text{dim\\_embd}}$, which is then multiplied with weight matrix $\\mathbf{W} \\in \\mathbb{R}^{3 \\cdot \\text{dim\\_embd}\\times\\text{dim\\_embd}}$, this multiplication follows $\\mathbb{R}^{1 \\times \\text{dim\\_embd}} \\times \\mathbb{R}^{\\text{dim\\_embd}\\times 3 \\cdot \\text{dim\\_embd}} \\rightarrow \\mathbb{R}^{1 \\times 3 \\cdot \\text{dim\\_embd}}$, the number of FLOPs is $2 \\cdot \\text{dim\\_embd} \\cdot 3 \\text{dim\\_embd} = 6 \\text{dim\\_embd}^{2}$. With $\\text{n\\_layers}$ layers and $\\text{n\\_heads}$ heads, there are $6 \\cdot \\text{n\\_layers} \\cdot \\text{n\\_heads} \\cdot \\text{dim\\_embd}^{2}$ FLOPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "The statement is wrong because per token, the cost of forward pass under the assumption that $\\text{dim\\_embd} >> \\text{context\\_length}$ is roughly $N$. And the backwork pass under the given condition costs roughly $2N$ per token.\n",
    "\n",
    "Therefore, the total cost for a single token is $3N$, which can also be justified with the previous subtasks. So the given statement is wrong."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
