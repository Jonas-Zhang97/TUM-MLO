{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "It could be the case that just by chance, the generated validation set correspondes perfectly to the trained model, meaning that the validation didn't represent the true performance of the model.\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "According to the lecture note we have\n",
    "\n",
    "$$\n",
    "\\text{P} \\left[ \\underset{k=1,\\cdots,K}{\\text{max}} \\left| \\hat{R} \\left(h_k, \\mathcal{D}_{val} \\right) - R\\left( h_k \\right) \\right| \\le O\\left( \\sqrt{\\frac{\\text{log} \\left( K / \\delta \\right)}{\\left| \\mathcal{D}_{val} \\right|}} \\right) \\right] \\ge 1- \\delta ,\n",
    "$$\n",
    "\n",
    "to correctly chose a $k$ without runging into problem, we can look at the term\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{\\text{log} \\left( K / \\delta \\right)}{\\left| \\mathcal{D}_{val} \\right|}},\n",
    "$$\n",
    "\n",
    "to obtain a validation error that gives a good estimate of the risk for all hyperparameter configurations, $\\text{log}(K)$ should be small relative to $\\left| \\mathcal{D}_{val} \\right|$, i.e.,\n",
    "\n",
    "$$\n",
    "\\text{log}(K) \\le \\left| \\mathcal{D}_{val} \\right| ~~~~~~ \\Rightarrow ~~~~~~ K \\le e^{\\left| \\mathcal{D}_{val} \\right|} .\n",
    "$$\n",
    "\n",
    "Let the $\\left| \\mathcal{D}_{val,small} \\right|$ for the small validation set be $a$ and therefore for the large validation set $\\left| \\mathcal{D}_{val,large} \\right| = 10 \\cdot a$. On the small validation set we can evaluate $e^a$ models and on the large validation set, $e^{10 \\cdot a}$ models can be tested.\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "### 1.\n",
    "\n",
    "Some features have larger deviation because of their nature, and during the ridge regression, these variables will receive larger penalty, which makes them more \"important\" than they should be.\n",
    "\n",
    "By scaling the variables, the effect of different variables on the final model is normalized.\n",
    "\n",
    "### 2.\n",
    "\n",
    "We can define the feature vector as $\\mathbf{\\theta}_{\\text{ridge}} = \\left[\\theta_0, \\theta_1, \\cdots, \\theta_N \\right] = \\left[ 0, \\mathbf{\\theta} \\right] \\in \\mathbb{R}^{N+1}$. Together with the given definition of $\\tilde{\\mathbf{X}}$, we have traditional ridge regeression as follows:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{2}\n",
    "  \\hat{\\mathbf{\\theta}}_{\\text{ridge}} &= \\text{arg} \\underset{\\mathbf{\\theta}_{\\text{ridge}}}{\\text{min}} \\left\\| \\mathbf{y} - \\tilde{\\mathbf{X}} \\mathbf{\\theta}_{\\text{ridge}} \\right\\|_2^2 + \\lambda \\left\\| \\mathbf{\\theta}_{\\text{ridge}} \\right\\|_2^2 \\\\\n",
    "  &= \\text{arg} \\underset{\\mathbf{\\theta}_{\\text{ridge}}}{\\text{min}} \\left\\| \\mathbf{y} - \\tilde{\\mathbf{X}} \\mathbf{\\theta}_{\\text{ridge}} \\right\\|_2^2 + \\lambda \\mathbf{\\theta}_{\\text{ridge}}^\\top \\mathbf{\\theta}_{\\text{ridge}}.\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "According to the task, $\\theta_0$ should not be considered for the penalty, to achieve this, we can multiply the penalty term with an identity, but starting with 0, denoted as $\\mathbf{A}$ i.e.,\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "0 &                         \\\\\n",
    "  & \\mathbf{I}_{N \\times N}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{N+1 \\times N+1},\n",
    "$$\n",
    "\n",
    "by doing this, the ridge regression problem can be rewritten as\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{\\theta}}_{\\text{ridge}} = \\text{arg} \\underset{\\mathbf{\\theta}_{\\text{ridge}}}{\\text{min}} \\left\\| \\mathbf{y} - \\tilde{\\mathbf{X}} \\mathbf{\\theta}_{\\text{ridge}} \\right\\|_2^2 + \\lambda \\mathbf{\\theta}_{\\text{ridge}}^\\top \\mathbf{A} \\mathbf{\\theta}_{\\text{ridge}}.\n",
    "$$\n",
    "\n",
    "We can replace the identity term $\\mathbf{I}$ in closed-form solution of ridge regression with the newly defined $\\mathbf{A}$, leading to\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6029005 , -0.59567545, -0.5285512 , ..., -0.52319133,\n",
       "         0.21335208,  1.07505242],\n",
       "       [ 0.51254171,  0.49225957,  0.72996619, ..., -0.25386267,\n",
       "         0.81996395, -0.9301872 ],\n",
       "       [ 0.62816682,  0.73648988,  0.95878753, ..., -0.74417894,\n",
       "        -0.84821868,  1.07505242],\n",
       "       ...,\n",
       "       [ 0.48533581,  0.40344855, -0.98619389, ..., -0.03978092,\n",
       "        -0.24160682, -0.9301872 ],\n",
       "       [ 1.15188054,  0.80309815, -0.29972986, ...,  0.08452461,\n",
       "         0.51665801, -0.9301872 ],\n",
       "       [ 1.54636621,  1.38036979, -0.29972986, ..., -0.79251998,\n",
       "        -0.84821868, -0.9301872 ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Load features\n",
    "X = np.loadtxt(\"hitters/hitters.x.csv\", delimiter = \",\", skiprows = 1)\n",
    "with open(\"hitters/hitters.x.csv\", \"r\") as f:\n",
    "  X_colnames = next(csv.reader(f))\n",
    "\n",
    "# Load salaries\n",
    "y = np.loadtxt(\"hitters/hitters.y.csv\", delimiter = \",\", skiprows = 1)\n",
    "\n",
    "# Normailize the feature vectors as discussed in subtask 1\n",
    "X -= np.mean(X, axis=0)\n",
    "X /= np.std(X, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
