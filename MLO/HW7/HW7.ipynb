{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "From the given plots of classifiers we can make the followiing statements: \n",
    "1. $h_1$ uses a linear boundary to make decision, the model is too simple for the distribution of data set, thus we are in the \"under-fitting region\" and we have bias problem, therefore the $\\hat{R}(h, D_{\\text{train}})$ and $\\hat{R}(h, D_{\\text{test}})$ should be both relatively large;\n",
    "2. by using high order polynomial we can obtain a classifier like $h_2$, where overfitting occurs, we have variance problem, thus the $\\hat{R}(h, D_{\\text{train}})$ shall be small but $\\hat{R}(h, D_{\\text{test}})$ would be large;\n",
    "3. $h_3$ is derived by low order polynomial, it can good represent the distribution of data set, so the $\\hat{R}(h, D_{\\text{train}})$ and $\\hat{R}(h, D_{\\text{test}})$ should be both small.\n",
    "\n",
    "Consider all the arguments above, we can answer the given question as interpreted in the following figure:\n",
    "\n",
    "![image](/home/yueyangzhang/Downloads/IMG_0013.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "First of all, $R(\\hat{h})$ should be decreasing as $n$ getting larger. With $\\text{inf}_{h\\in \\mathcal{H}} R(h)$ stands for the minimum possible risk that can be achieved by $h\\in \\mathcal{H}$, the estimation error should getting smaller w.r.t. the increase of $n$.\n",
    "\n",
    "Subsequently, $\\text{inf}_{h\\in \\mathcal{H}} R(h)$ and $R(h^*)$ are both independent with $n$, so in the plot the approximation error should be constant.\n",
    "\n",
    "Finally, we have the following equation for the excess error\n",
    "$$\n",
    "R(\\hat{h}) - R(h^*) = (R(\\hat{h}) - \\text{inf}_{h\\in \\mathcal{H}} R(h)) + (\\text{inf}_{h\\in \\mathcal{H}} R(h) - R(h^*)),\n",
    "$$\n",
    "\n",
    "with the term $\\text{inf}_{h\\in \\mathcal{H}} R(h) - R(h^*)$ being a constant, the excess error is just the estimation error shifted by approximation error.\n",
    "\n",
    "Therefore, we get the following plot\n",
    "\n",
    "![image](/home/yueyangzhang/Downloads/IMG_0012.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "In the lecture note, we have the following equation\n",
    "\n",
    "$$\n",
    "R(\\mathbf{\\theta}) \\le \\hat{R}(\\mathbf{\\theta}) + B \\sqrt{\\frac{\\text{log}(\\frac{|\\mathcal{H}|}{\\delta})}{2\\cdot n}}\n",
    "$$\n",
    "\n",
    "Given $\\mathbf{\\theta} \\in \\mathbb{R}^{p}$ and each entry of it is either $1$ or $-1$ we have $|\\mathcal{H}| = 2^p$, and since we are using 0-1 loss, so the loss is bounded by 1, which means $B=1$, therefore we have\n",
    "$$\n",
    "R(\\mathbf{\\theta}) \\le \\hat{R}(\\mathbf{\\theta}) + \\sqrt{\\frac{\\text{log}(\\frac{2^p}{\\delta})}{2\\cdot n}}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
