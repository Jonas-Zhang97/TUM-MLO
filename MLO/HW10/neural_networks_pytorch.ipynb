{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CIFAR10 train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the computational complexity of a single step of stochastic gradient we can look at 3 major steps\n",
    "\n",
    "**1. Forward Passes**\n",
    "\n",
    "Firstly, the computation from input layer to the first hiddden layer involves k many dot multiplication between vectors in $\\mathbb{R}^{d}$, which gives us $\\mathcal{O}(kd)$.\n",
    "\n",
    "Then, in the calculation between 2 hidden layers, there are $k^2$ many scaler products, and there are q many hidden layers, which gives us $\\mathcal{O}(qk^2)$.\n",
    "\n",
    "Lastly, $k$ neurons of the last layer will be mapped to a single scaler output, it costs $\\mathcal{O}(k)$.\n",
    "\n",
    "We can obtain a total cost by summing them together, which leads to $\\mathcal{O}((d + 1)k + qk^2)$.\n",
    "\n",
    "**2. Backpropagation**\n",
    "\n",
    "Firstly, the computation from output layer to an arbitary neuron in the last hidden layer contains 2 gradient, namely:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_{qi}} ~~~~~~ \\text{and} ~~~~~~ \\frac{\\partial l}{\\partial y},\n",
    "$$\n",
    "\n",
    "it has a constant cost $\\mathcal{O}(2)$, and there are $k$ many neurons, so the cost is $\\mathcal{O}(k)$.\n",
    "\n",
    "Subsequently, between 2 hidden layers, we have $k^2$ many calculations interpreted above, the cost goes $\\mathcal{O}(k^2)$, and multiplied with $q$ layers, we have the cost in all hidden layers: $\\mathcal{O}(qk^2)$.\n",
    "\n",
    "By summing them together we have $\\mathcal{O}(k + qk^2)$.\n",
    "\n",
    "**3. Parameter Updating**\n",
    "\n",
    "After the calculation, we need to update the parameters in the fcnnwork, in the input layer we have $dk$ many parameters, in $q$ layers there are $q \\cdot k^2$ parameters, and in the input layer we have $k$ parameters. To update them, the cost is $\\mathcal{O}((d+1)k+qk^2)$.\n",
    "\n",
    "**Total Cost**\n",
    "\n",
    "The total cost can be calculated by adding all the 3 steps together, thus we have $\\mathcal{O}(3qk^2 + 2(d+1)k + k)$, by ignoring constant coeffecients and lower order terms, it results in $\\mathcal{O}(qk^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "Here, let's filter dataset by its name and initialize it in batch with size 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['cat', 'dog', 'ship']\n",
    "class_indices = {'cat': 0, 'dog': 1, 'ship': 2}\n",
    "\n",
    "def filter_classes(dataset, classes):\n",
    "    class_to_idx = {dataset.classes[i]: i for i in range(len(dataset.classes))}\n",
    "    filtered_indices = []\n",
    "    labels = []\n",
    "    for i, (_, label) in enumerate(dataset):\n",
    "        if dataset.classes[label] in classes:\n",
    "            filtered_indices.append(i)\n",
    "            labels.append(class_indices[dataset.classes[label]])\n",
    "    return torch.utils.data.Subset(dataset, filtered_indices), torch.tensor(labels)\n",
    "\n",
    "trainset_filtered, train_labels = filter_classes(trainset, class_names)\n",
    "testset_filtered, test_labels = filter_classes(testset, class_names)\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, labels):\n",
    "        self.subset = subset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.subset[idx]\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "train_dataset = CustomDataset(trainset_filtered, train_labels)\n",
    "test_dataset = CustomDataset(testset_filtered, test_labels)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified to include only cat, dog, and ship classes\n",
    "class_names = ['cat', 'dog', 'ship']\n",
    "class_indices = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "# Function to filter out only the specified classes\n",
    "def filter_classes(dataset, classes):\n",
    "    class_to_idx = {dataset.classes[i]: i for i in range(len(dataset.classes))}\n",
    "    indices = [i for i, (_, label) in enumerate(dataset) if dataset.classes[label] in classes]\n",
    "    return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "# Apply the filter to the train and test datasets\n",
    "trainset_filtered = filter_classes(trainset, class_names)\n",
    "testset_filtered = filter_classes(testset, class_names)\n",
    "\n",
    "# Initialize the dataloaders\n",
    "trainloader = DataLoader(trainset_filtered, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset_filtered, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "The FCNN class is implemented as our fully connected neural fcnnwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNN, self).__init__()\n",
    "        # Hidden layer with d=3*32*32 and 512 neurons\n",
    "        self.layer1 = nn.Linear(3 * 32 * 32, 512)\n",
    "        # Output layer maps number of neurons to the output dimension\n",
    "        self.layer2 = nn.Linear(512, len(class_names))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3 * 32 * 32)\n",
    "        x = F.relu(self.layer1(x)) \n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "fcnn = FCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "Here, we define an SGD optimizer with CrossEntropyLoss, and perform training on our setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(fcnn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: %s tensor([8, 8, 3, 3])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 8 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m best_model_state\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Train the fcnnwork and save the best model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m best_model_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[1;32m     47\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_model_state, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(fcnn, trainloader, testloader, optimizer, criterion, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m fcnn(inputs)\n\u001b[0;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 8 is out of bounds."
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(fcnn, trainloader, testloader, optimizer, criterion, epochs=10):\n",
    "  best_accuracy = 0\n",
    "  best_model_state = None\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    fcnn.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "      inputs, labels = data\n",
    "      print(\"label: %s\", labels)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = fcnn(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    # Evaluate on test data\n",
    "    fcnn.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "      for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = fcnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}, Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "    # Save the best model\n",
    "    if test_accuracy > best_accuracy:\n",
    "      best_accuracy = test_accuracy\n",
    "      best_model_state = fcnn.state_dict()\n",
    "\n",
    "  print('Training Finished')\n",
    "  return best_model_state\n",
    "\n",
    "# Train the fcnnwork and save the best model\n",
    "best_model_state = train_and_evaluate(fcnn, trainloader, testloader, optimizer, criterion)\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_model_state, './best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "We shall load the best model and report the test accuracy for overall and per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of cat : 68.7%\n",
      "Accuracy of dog : 52.7%\n",
      "Accuracy of ship : 88.0%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(fcnn, testloader, class_names):\n",
    "  fcnn.eval()\n",
    "  class_correct = list(0. for i in range(len(class_names)))\n",
    "  class_total = list(0. for i in range(len(class_names)))\n",
    "  with torch.no_grad():\n",
    "    for data in testloader:\n",
    "      images, labels = data\n",
    "      outputs = fcnn(images)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      c = (predicted == labels).squeeze()\n",
    "      for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "  for i in range(len(class_names)):\n",
    "    print(f'Accuracy of {class_names[i]} : {100 * class_correct[i] / class_total[i]}%')\n",
    "\n",
    "# Load the best model\n",
    "fcnn.load_state_dict(torch.load('./best_model.pth'))\n",
    "\n",
    "# Evaluate the best model\n",
    "evaluate_model(fcnn, testloader, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can justify that the model performs better when classifing a ship, the reason for it is highly likely that the different between ship and 2 animals we have is significantly larger, so it is easier to distinguish ships from the other. However, because of the similarity of cats and dogs, the model can make many error when labeling them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
